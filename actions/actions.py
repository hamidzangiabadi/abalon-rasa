from rasa_sdk import Action, Tracker
from rasa_sdk.executor import CollectingDispatcher
from llama_cpp import Llama
import warnings

warnings.filterwarnings("ignore")

print("ğŸ”„ Loading LLM model...")

llm = Llama(
    model_path="./maral-model/Maral-7B-alpha-1-Q4_K_M.gguf",
    n_ctx=1024,      # Back to reasonable size
    n_batch=128,     # Back to working size
    n_threads=6,
    n_gpu_layers=0,
    f16_kv=True,     # Back on for stability
    use_mlock=True,
    verbose=False
)

class ActionLocalLLMResponse(Action):
    def name(self):
        return "action_local_llm_response"

    def run(self, dispatcher: CollectingDispatcher,
            tracker: Tracker,
            domain: dict):

        user_input = tracker.latest_message.get('text', '').strip()
        print(f"ğŸ“¥ User: {user_input}")

        # Simple but effective prompt
        prompt = f"""Ø´Ù…Ø§ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¢Ø¨Ø§Ù„ÙˆÙ† Ú©Ù„ÙˆØ¯ Ù‡Ø³ØªÛŒØ¯. Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ø¯Ù‡ÛŒØ¯.

Ú©Ø§Ø±Ø¨Ø±: {user_input}
Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¢Ø¨Ø§Ù„ÙˆÙ†:"""

        try:
            response = llm(
                prompt, 
                max_tokens=100,
                temperature=0.3,
                stop=["Ú©Ø§Ø±Ø¨Ø±:", "Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¢Ø¨Ø§Ù„ÙˆÙ†:"],
                echo=False
            )
            
            reply = response["choices"][0]["text"].strip()
            
            # Simple validation
            if not reply or len(reply) < 5:
                reply = "Ø³Ù„Ø§Ù…! Ù…Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø¢Ø¨Ø§Ù„ÙˆÙ† Ú©Ù„ÙˆØ¯ Ù‡Ø³ØªÙ…. Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú©ØªÙˆÙ† Ú©Ù†Ù…ØŸ"

            dispatcher.utter_message(reply)
            print(f"âœ… Bot: {reply}")

        except Exception as e:
            print(f"âŒ Error: {e}")
            dispatcher.utter_message("Ø³Ù„Ø§Ù…! Ù…Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø¢Ø¨Ø§Ù„ÙˆÙ† Ú©Ù„ÙˆØ¯ Ù‡Ø³ØªÙ…. Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú©ØªÙˆÙ† Ú©Ù†Ù…ØŸ")

        return []
